{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:21.606420Z",
     "iopub.status.busy": "2022-05-31T14:25:21.605880Z",
     "iopub.status.idle": "2022-05-31T14:25:21.618517Z",
     "shell.execute_reply": "2022-05-31T14:25:21.617439Z",
     "shell.execute_reply.started": "2022-05-31T14:25:21.606374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing  import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining all the preprocessing function that i needed before final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:23.869066Z",
     "iopub.status.busy": "2022-05-31T14:25:23.868302Z",
     "iopub.status.idle": "2022-05-31T14:25:23.883057Z",
     "shell.execute_reply": "2022-05-31T14:25:23.882104Z",
     "shell.execute_reply.started": "2022-05-31T14:25:23.869031Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_stopwords(text):\n",
    "    '''this function will count total number of stop words in text and title column'''\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    c_stopwords = [w for w in word_tokens if w in stop_words]\n",
    "    \n",
    "    return len(c_stopwords)\n",
    "\n",
    "def count_unique_words(text):\n",
    "    '''this function will count total number of unique words in text and title column'''\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    unique_words=[]\n",
    "    for i in range(len(word_tokens)):\n",
    "        if word_tokens[i] not in unique_words:\n",
    "            unique_words.append(word_tokens[i])    \n",
    "    return len(unique_words)\n",
    "\n",
    "\n",
    "def data_preprocess(text):\n",
    "    '''This function will will preprocess the data by removing the puchuation digit email address and all non alphabet wrods from text.'''\n",
    "    \n",
    "    final_text=text.lower() \n",
    "    final_text=re.sub(r\"[A-Za-z\\d\\-\\.]+@[A-Za-z\\.-]+\\b\", \" \", final_text) #remove the email address from text\n",
    "    final_text = re.sub(r'http\\S+', '', final_text) # remove http links from text\n",
    "    final_text=re.sub(r\"\\d+\", \" \", final_text) # remove any digit from text\n",
    "    final_text=re.sub(r\"[^a-zA-Z]+\", \" \", final_text) # remove anything except Alphabets from text\n",
    "   \n",
    "    #removing the punchuation\n",
    "    punctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\n",
    "    no_punc = \" \"\n",
    "    for char in final_text:\n",
    "        if char not in punctuations:\n",
    "            no_punc = no_punc + char.lower()\n",
    "        else:\n",
    "            no_punc+=\" \"  #if punchuation found add space\n",
    "    final_text=no_punc.strip()\n",
    "    return final_text\n",
    "\n",
    "CONTRACTION_MAP=pickle.load(open('../input/fake-news-case-study-preprocessed-daa/CONTRACTION_MAP.pkl','rb'))# loading the contraction map \n",
    "\n",
    "def decontracted(text):\n",
    "    '''this function will Replace all apostrophe/short words from text data'''\n",
    "    for word in text.split():\n",
    "        if word.lower() in CONTRACTION_MAP:\n",
    "            text = text.replace(word, CONTRACTION_MAP[word.lower()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''This function will remove stopwords from text data.'''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    filtered_sentence = \" \".join([w for w in word_tokens if not w.lower() in stop_words])\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Defining the function_1  which will take input here input can be single or set of datapoint and return the prediction of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:26.370775Z",
     "iopub.status.busy": "2022-05-31T14:25:26.370214Z",
     "iopub.status.idle": "2022-05-31T14:25:26.389535Z",
     "shell.execute_reply": "2022-05-31T14:25:26.388415Z",
     "shell.execute_reply.started": "2022-05-31T14:25:26.370741Z"
    }
   },
   "outputs": [],
   "source": [
    "def function_1(test):\n",
    "    \"\"\"this function will take dataset as input(single of set of datapoint) and perfrom data preprocessing and return the predictions as output\"\"\"\n",
    "    df_data=test.copy(deep=True)  \n",
    "    \n",
    "    #filling nan value using ffil method\n",
    "    df_data.fillna(method=\"ffill\",inplace=True)\n",
    "          \n",
    "    #Droping the duplicates\n",
    "    df_data.drop_duplicates(keep='first',inplace=True)\n",
    "    \n",
    "    # creating new features from text data\n",
    "    df_data['num_characters_title'] = df_data['title'].apply(len)\n",
    "    df_data['num_characters_text'] = df_data['text'].apply(len)\n",
    "    \n",
    "    df_data['num_word_title'] = df_data['title'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "    df_data['num_word_text'] = df_data['text'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "    \n",
    "    df_data['num_sentences_title'] = df_data['title'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "    df_data['num_sentences_text'] = df_data['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "    \n",
    "    df_data['Count_unique_words_title'] = df_data['title'].apply(lambda x:count_unique_words(x))\n",
    "    df_data['Count_unique_words_text'] = df_data['text'].apply(lambda x:count_unique_words(x))\n",
    "    \n",
    "    df_data['Count_Stop_words_title'] = df_data['title'].apply(lambda x:count_stopwords(x))\n",
    "    df_data['Count_Stop_words_text'] = df_data['text'].apply(lambda x:count_stopwords(x))\n",
    "    \n",
    "    #Calculating average word length\n",
    "    #This can be calculated by dividing the counts of characters by counts of words.\n",
    "    df_data['Avg_word_length_title'] = df_data['num_characters_title']/df_data[\"num_word_title\"]\n",
    "    df_data['Avg_word_length_text'] = df_data['num_characters_text']/df_data[\"num_word_text\"]\n",
    "    \n",
    "    #Calculating average sentence length\n",
    "    #This can be calculated by dividing the counts of words by the counts of sentences.\n",
    "    df_data['Avg_sentence_length_title'] = df_data['num_word_title']/df_data[\"num_sentences_title\"]\n",
    "    df_data['Avg_sentence_length_text'] = df_data['num_word_text']/df_data[\"num_sentences_text\"]\n",
    "    \n",
    "    #Stopwords count vs words counts Ratio\n",
    "    #This feature is also the ratio of counts of stopwords to the total number of words.\n",
    "    df_data['Stopword_count_ratio_title'] = df_data['Count_Stop_words_title']/df_data[\"num_word_title\"]\n",
    "    df_data['Stopword_count_ratio_text'] = df_data['Count_Stop_words_text']/df_data[\"num_word_text\"]\n",
    "    \n",
    "    #unique words vs word count Ratio\n",
    "    #This feature is basically the ratio of unique words to a total number of words.\n",
    "    df_data['Unique_words_count_ratio_title'] = df_data['Count_unique_words_title']/df_data[\"num_word_title\"]\n",
    "    df_data['Unique_words_count_ratio_text'] = df_data['Count_unique_words_text']/df_data[\"num_word_text\"]\n",
    "    \n",
    "    #To avoud any +/- inf nan values issue \n",
    "    df_data = df_data.replace([np.inf, -np.inf, -0,np.nan], 0)#fix for infiniy neg issue\n",
    "    \n",
    "    # performing the decontraction on the dataset\n",
    "    df_data['cleaned_text'] = df_data.apply(lambda x: decontracted(x[\"text\"]),axis=1)\n",
    "    df_data['cleaned_title'] = df_data.apply(lambda x: decontracted(x[\"title\"]),axis=1)\n",
    "    \n",
    "     #apply preprocessing on the dataset\n",
    "    df_data['cleaned_text'] = df_data.apply(lambda x: data_preprocess(x[\"cleaned_text\"]),axis=1)\n",
    "    df_data['cleaned_title'] = df_data.apply(lambda x: data_preprocess(x[\"cleaned_title\"]),axis=1)\n",
    "    \n",
    "    df_data[\"Without_Stopwords_text\"]=df_data.apply(lambda x:remove_stopwords(x[\"cleaned_text\"]),axis=1)\n",
    "    df_data[\"Without_Stopwords_title\"]=df_data.apply(lambda x:remove_stopwords(x[\"cleaned_title\"]),axis=1)\n",
    "    \n",
    "    # fetching the required features in our vairable X\n",
    "    X=df_data.drop([\"id\",\"title\",\"text\",\"cleaned_text\",\"cleaned_title\",\"author\"], axis=1, inplace=False)\n",
    "    \n",
    "    #loading the standard scaler\n",
    "    Std_Scaler=pickle.load(open('../input/fake-news-case-study-preprocessed-daa/Std_Scaler.pkl','rb'))\n",
    "    \n",
    "    #fethcing the numerical features \n",
    "    df_Num_Ft=X.drop([\"Without_Stopwords_text\",\"Without_Stopwords_title\"],axis=1,inplace=False)\n",
    "    \n",
    "    #standardizing the numerical features\n",
    "    df_Num_Std=Std_Scaler.transform(df_Num_Ft)\n",
    "    \n",
    "    #loading the tfidf vectorizer \n",
    "    vectorizer_text_tfidf=pickle.load(open('../input/fake-news-case-study-preprocessed-daa/vectorizer_text_tfidf.pkl','rb'))\n",
    "    vectorizer_title_tfdf=pickle.load(open('../input/fake-news-case-study-preprocessed-daa/vectorizer_title_tfidf.pkl','rb'))\n",
    "    \n",
    "    #vectorizing the both title and text using tfidf vectorizer\n",
    "    df_title_tfidf = vectorizer_title_tfdf.transform(X['Without_Stopwords_title'].values).toarray()\n",
    "    df_text_tfidf = vectorizer_text_tfidf.transform(X['Without_Stopwords_text'].values).toarray()\n",
    "    \n",
    "    # stacking all the features for train and test dataset\n",
    "    df_final = np.hstack((df_title_tfidf, df_text_tfidf,df_Num_Std))\n",
    "    \n",
    "    #loading the our best model which is voting classifer \n",
    "    Voting_clf = pickle.load(open('../input/fake-news-case-study-preprocessed-daa/voting.pkl', 'rb'))\n",
    "    \n",
    "    #doing the prediction on final data\n",
    "    y_test_pred_voting = Voting_clf.predict(df_final)\n",
    "    \n",
    "    return y_test_pred_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the testdata set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:30.155862Z",
     "iopub.status.busy": "2022-05-31T14:25:30.155481Z",
     "iopub.status.idle": "2022-05-31T14:25:30.491569Z",
     "shell.execute_reply": "2022-05-31T14:25:30.490524Z",
     "shell.execute_reply.started": "2022-05-31T14:25:30.155829Z"
    }
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"../input/case-studyfake-news-dataset/test.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:31.578257Z",
     "iopub.status.busy": "2022-05-31T14:25:31.577808Z",
     "iopub.status.idle": "2022-05-31T14:25:31.584697Z",
     "shell.execute_reply": "2022-05-31T14:25:31.583763Z",
     "shell.execute_reply.started": "2022-05-31T14:25:31.578221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5200, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing the single data point to our function_1 and getting the prediction for that data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:36.577815Z",
     "iopub.status.busy": "2022-05-31T14:25:36.577433Z",
     "iopub.status.idle": "2022-05-31T14:25:36.784394Z",
     "shell.execute_reply": "2022-05-31T14:25:36.783333Z",
     "shell.execute_reply.started": "2022-05-31T14:25:36.577785Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=function_1(test.iloc[[0],:])#test.iloc[[0] this will give single row from test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:38.666110Z",
     "iopub.status.busy": "2022-05-31T14:25:38.665076Z",
     "iopub.status.idle": "2022-05-31T14:25:38.671916Z",
     "shell.execute_reply": "2022-05-31T14:25:38.670810Z",
     "shell.execute_reply.started": "2022-05-31T14:25:38.666068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing the whole test dataset to our function_1 and getting the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:25:41.624362Z",
     "iopub.status.busy": "2022-05-31T14:25:41.623654Z",
     "iopub.status.idle": "2022-05-31T14:28:10.425096Z",
     "shell.execute_reply": "2022-05-31T14:28:10.423972Z",
     "shell.execute_reply.started": "2022-05-31T14:25:41.624324Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=function_1(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# got the prediction for 5200 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:31:11.447581Z",
     "iopub.status.busy": "2022-05-31T14:31:11.447155Z",
     "iopub.status.idle": "2022-05-31T14:31:11.454584Z",
     "shell.execute_reply": "2022-05-31T14:31:11.453427Z",
     "shell.execute_reply.started": "2022-05-31T14:31:11.447545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:31:14.197026Z",
     "iopub.status.busy": "2022-05-31T14:31:14.196644Z",
     "iopub.status.idle": "2022-05-31T14:31:14.203837Z",
     "shell.execute_reply": "2022-05-31T14:31:14.203190Z",
     "shell.execute_reply.started": "2022-05-31T14:31:14.196994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the function which will take dataset with X and Y values and return the accuracy on same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:32:33.947025Z",
     "iopub.status.busy": "2022-05-31T14:32:33.946381Z",
     "iopub.status.idle": "2022-05-31T14:32:34.102030Z",
     "shell.execute_reply": "2022-05-31T14:32:34.100934Z",
     "shell.execute_reply.started": "2022-05-31T14:32:33.946984Z"
    }
   },
   "outputs": [],
   "source": [
    "def function_2(test):\n",
    "    \"\"\"this function will take dataset (with x and y) as input and perfrom data preprocessing and return the accruacy of model\"\"\"\n",
    "    \n",
    "    df_data=test.copy(deep=True) # creating deep copy or orignal dataset  \n",
    "    \n",
    "    #dropping the NAN rows from dataset\n",
    "    df_data.dropna(inplace=True)\n",
    "    df_data.reset_index(drop=True,inplace=True)  \n",
    "          \n",
    "    #Droping the duplicates\n",
    "    df_data.drop_duplicates(keep='first',inplace=True)\n",
    "    \n",
    "    # creating new features from text data\n",
    "    df_data['num_characters_title'] = df_data['title'].apply(len)\n",
    "    df_data['num_characters_text'] = df_data['text'].apply(len)\n",
    "    \n",
    "    df_data['num_word_title'] = df_data['title'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "    df_data['num_word_text'] = df_data['text'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "    \n",
    "    df_data['num_sentences_title'] = df_data['title'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "    df_data['num_sentences_text'] = df_data['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "    \n",
    "    df_data['Count_unique_words_title'] = df_data['title'].apply(lambda x:count_unique_words(x))\n",
    "    df_data['Count_unique_words_text'] = df_data['text'].apply(lambda x:count_unique_words(x))\n",
    "    \n",
    "    df_data['Count_Stop_words_title'] = df_data['title'].apply(lambda x:count_stopwords(x))\n",
    "    df_data['Count_Stop_words_text'] = df_data['text'].apply(lambda x:count_stopwords(x))\n",
    "    \n",
    "    #Calculating average word length\n",
    "    #This can be calculated by dividing the counts of characters by counts of words.\n",
    "    df_data['Avg_word_length_title'] = df_data['num_characters_title']/df_data[\"num_word_title\"]\n",
    "    df_data['Avg_word_length_text'] = df_data['num_characters_text']/df_data[\"num_word_text\"]\n",
    "    \n",
    "    #Calculating average sentence length\n",
    "    #This can be calculated by dividing the counts of words by the counts of sentences.\n",
    "    df_data['Avg_sentence_length_title'] = df_data['num_word_title']/df_data[\"num_sentences_title\"]\n",
    "    df_data['Avg_sentence_length_text'] = df_data['num_word_text']/df_data[\"num_sentences_text\"]\n",
    "    \n",
    "    #Stopwords count vs words counts Ratio\n",
    "    #This feature is also the ratio of counts of stopwords to the total number of words.\n",
    "    df_data['Stopword_count_ratio_title'] = df_data['Count_Stop_words_title']/df_data[\"num_word_title\"]\n",
    "    df_data['Stopword_count_ratio_text'] = df_data['Count_Stop_words_text']/df_data[\"num_word_text\"]\n",
    "    \n",
    "    #unique words vs word count Ratio\n",
    "    #This feature is basically the ratio of unique words to a total number of words.\n",
    "    df_data['Unique_words_count_ratio_title'] = df_data['Count_unique_words_title']/df_data[\"num_word_title\"]\n",
    "    df_data['Unique_words_count_ratio_text'] = df_data['Count_unique_words_text']/df_data[\"num_word_text\"]\n",
    "    \n",
    "    #To avoud any +/- inf nan values issue \n",
    "    df_data = df_data.replace([np.inf, -np.inf, -0,np.nan], 0)#fix for infiniy neg issue\n",
    "    \n",
    "    # performing the decontraction on the dataset\n",
    "    df_data['cleaned_text'] = df_data.apply(lambda x: decontracted(x[\"text\"]),axis=1)\n",
    "    df_data['cleaned_title'] = df_data.apply(lambda x: decontracted(x[\"title\"]),axis=1)\n",
    "    \n",
    "     #apply preprocessing on the dataset\n",
    "    df_data['cleaned_text'] = df_data.apply(lambda x: data_preprocess(x[\"cleaned_text\"]),axis=1)\n",
    "    df_data['cleaned_title'] = df_data.apply(lambda x: data_preprocess(x[\"cleaned_title\"]),axis=1)\n",
    "    \n",
    "    df_data[\"Without_Stopwords_text\"]=df_data.apply(lambda x:remove_stopwords(x[\"cleaned_text\"]),axis=1)\n",
    "    df_data[\"Without_Stopwords_title\"]=df_data.apply(lambda x:remove_stopwords(x[\"cleaned_title\"]),axis=1)\n",
    "    \n",
    "    #splitting class labels and all the features \n",
    "    Y=df_data[\"label\"]\n",
    "    X=df_data.drop([\"id\",\"title\",\"text\",\"label\",\"cleaned_text\",\"cleaned_title\",\"author\"], axis=1, inplace=False)\n",
    "    \n",
    "    # performing train test split 25% data for testnig and 75% data for training the model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, stratify=Y,random_state=42)\n",
    "    \n",
    "    X_train_Num_Ft=X_train.drop([\"Without_Stopwords_text\",\"Without_Stopwords_title\"],axis=1,inplace=False)\n",
    "    X_test_Num_Ft=X_test.drop([\"Without_Stopwords_text\",\"Without_Stopwords_title\"],axis=1,inplace=False)\n",
    "    \n",
    "    #initializing the standdard scaler for numberical features\n",
    "    Std_Scaler=StandardScaler()\n",
    "    \n",
    "    X_train_Num_Std=Std_Scaler.fit_transform(X_train_Num_Ft)\n",
    "    X_test_Num_Std=Std_Scaler.transform(X_test_Num_Ft)\n",
    "    \n",
    "    #vectorizing the title features using tfidf vectorizer\n",
    "    vectorizer_title_tfidf =TfidfVectorizer(max_features=3500)\n",
    "    vectorizer_title_tfidf.fit(X_train[\"Without_Stopwords_title\"].values)\n",
    "\n",
    "    # we use the fitted tfidfVectorizer to convert the text to vector\n",
    "    X_train_title_tfidf = vectorizer_title_tfidf.transform(X_train['Without_Stopwords_title']).toarray()\n",
    "    X_test_title_tfidf = vectorizer_title_tfidf.transform(X_test['Without_Stopwords_title']).toarray()\n",
    "    \n",
    "    #vectorizing the text features using tfidf vectorizer\n",
    "    vectorizer_text_tfidf =TfidfVectorizer(max_features=4500)\n",
    "    vectorizer_text_tfidf.fit(X_train[\"Without_Stopwords_text\"].values)\n",
    "\n",
    "    # we use the fitted countVectorizer to convert the text to vector\n",
    "    X_train_text_tfidf = vectorizer_text_tfidf.transform(X_train['Without_Stopwords_text'].values).toarray()\n",
    "    X_test_text_tfidf = vectorizer_text_tfidf.transform(X_test['Without_Stopwords_text'].values).toarray()\n",
    "    \n",
    "    # stacking all the features for train and test dataset \n",
    "    X_train_final_tfidf = np.hstack((X_train_title_tfidf, X_train_text_tfidf,X_train_Num_Std))\n",
    "    X_test_final_tfidf = np.hstack((X_test_title_tfidf , X_test_text_tfidf,X_test_Num_Std))\n",
    "    \n",
    "    #loading our best model which is voting classifier\n",
    "    Voting_clf = pickle.load(open('../input/fake-news-case-study-preprocessed-daa/voting.pkl', 'rb'))\n",
    "    \n",
    "    #fitting on train dataset\n",
    "    Voting_clf.fit(X_train_final_tfidf,y_train)\n",
    "    #performing the prediciton on test dataset\n",
    "    y_pred = Voting_clf.predict(X_test_final_tfidf)\n",
    "    #calculating the accuracy on testdataset\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:32:39.990290Z",
     "iopub.status.busy": "2022-05-31T14:32:39.989869Z",
     "iopub.status.idle": "2022-05-31T14:32:42.877509Z",
     "shell.execute_reply": "2022-05-31T14:32:42.876414Z",
     "shell.execute_reply.started": "2022-05-31T14:32:39.990252Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"../input/case-studyfake-news-dataset/train.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:32:45.771319Z",
     "iopub.status.busy": "2022-05-31T14:32:45.770912Z",
     "iopub.status.idle": "2022-05-31T14:32:45.777215Z",
     "shell.execute_reply": "2022-05-31T14:32:45.776178Z",
     "shell.execute_reply.started": "2022-05-31T14:32:45.771285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling function_2 and passing whole train dataset and getting the accuracy as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:32:48.014897Z",
     "iopub.status.busy": "2022-05-31T14:32:48.014089Z",
     "iopub.status.idle": "2022-05-31T14:43:49.663107Z",
     "shell.execute_reply": "2022-05-31T14:43:49.662274Z",
     "shell.execute_reply.started": "2022-05-31T14:32:48.014851Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy=function_2(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Got the 98.6% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T14:45:18.284728Z",
     "iopub.status.busy": "2022-05-31T14:45:18.284347Z",
     "iopub.status.idle": "2022-05-31T14:45:18.291065Z",
     "shell.execute_reply": "2022-05-31T14:45:18.290173Z",
     "shell.execute_reply.started": "2022-05-31T14:45:18.284698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860017497812773"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
